{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full FineTuning, LoRa and QLoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 4, 'text': \"dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\"}\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Get dataset\n",
    "dataset=load_dataset(\"yelp_review_full\")\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Tokenize data\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load pretrained model from huggingface\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cost reasons I trained model on smaller dataset\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(20000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Model Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    auto_find_batch_size=True,\n",
    "    num_train_epochs=3, #increase for more accuracy\n",
    "    logging_steps=100,\n",
    "    report_to = \"none\",\n",
    "    output_dir=\"test_trainer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Define metrics with which I will evaluate the models\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Create the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 48:58]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.6747997999191284,\n",
       " 'eval_model_preparation_time': 0.0024,\n",
       " 'eval_accuracy': 0.2125,\n",
       " 'eval_f1': 0.10092274126877032,\n",
       " 'eval_precision': 0.13207291593177292,\n",
       " 'eval_recall': 0.19972638833244546,\n",
       " 'eval_runtime': 26.5371,\n",
       " 'eval_samples_per_second': 75.366,\n",
       " 'eval_steps_per_second': 9.421}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model before training\n",
    "trainer.evaluate() # Model currently guesses randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7500/7500 41:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.396000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.243900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.122600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.073400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.096700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.082800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.057400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.040200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.039700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.041400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.125700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.050300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.971100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.951900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.946900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.026400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.979600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.947600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.998800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.983900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.991400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.924000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.985900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.913600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.767800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.814300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.724500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.740800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.735700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.784500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.782000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.741900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.723800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.775500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.779300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.716300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.741900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.809400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.751700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.757400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.739100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.744200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.756200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.703800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.746500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.703900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.713800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.701400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.731300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.496600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.492900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.465600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.464500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.466700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.473600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.456500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.446600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.457800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.471900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.493200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.427800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.463500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.530900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.442300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.419200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.499900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.449500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.396400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.447400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.431900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.467900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.415800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.452500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.373400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7500, training_loss=0.7478029520670573, metrics={'train_runtime': 2519.0307, 'train_samples_per_second': 23.819, 'train_steps_per_second': 2.977, 'total_flos': 1.578708854784e+16, 'train_loss': 0.7478029520670573, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained('./fine-tuned-model')\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('./fine-tuned-model', num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1/250 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.1167843341827393,\n",
       " 'eval_model_preparation_time': 0.0022,\n",
       " 'eval_accuracy': 0.639,\n",
       " 'eval_f1': 0.6403095857071527,\n",
       " 'eval_precision': 0.6420707143936288,\n",
       " 'eval_recall': 0.6392827657775471,\n",
       " 'eval_runtime': 27.3998,\n",
       " 'eval_samples_per_second': 72.993,\n",
       " 'eval_steps_per_second': 9.124}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model after training with eval_dataset\n",
    "fullfinetune_metrics = trainer.evaluate()\n",
    "fullfinetune_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.22425028681755066,\n",
       " 'eval_model_preparation_time': 0.0024,\n",
       " 'eval_accuracy': 0.9294,\n",
       " 'eval_f1': 0.9294323378397102,\n",
       " 'eval_precision': 0.9298401691306175,\n",
       " 'eval_recall': 0.9292947086966195,\n",
       " 'eval_runtime': 286.0499,\n",
       " 'eval_samples_per_second': 69.918,\n",
       " 'eval_steps_per_second': 8.74,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check for overfitting (It is overfitted)\n",
    "trainer.evaluate(eval_dataset=small_train_dataset) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0, 'text': 'Just called this location and I live 1.8 miles away. I asked them to deliver and they informed me that they would not deliver to my house because it was a couple hundred yards out of the map plan. They asked me to call the power and southern store. This store advised me that they could not deliver because jimmy johns has a two mile radius they can deliver to.  Called this store back and they once again decided to tell me even though I was in the two mile radius they did not want to deliver to me and my only option was for pickup. I will never eat at this location. I know the owners at Firehouse Subs and they go out of the way and this location is just lazy. Not getting my money jimmy johns no matter how fast you are. Laziness is worse'}\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 6.1238,  1.1176, -1.4090, -2.8088, -2.0002]], device='cuda:0'), hidden_states=None, attentions=None)\n",
      "Prediction for random sample: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device) # move model to GPU\n",
    "\n",
    "random_sample = dataset['train'][random.randint(0, len(dataset['train']) - 1)] # Get random sample from training data\n",
    "print(random_sample)\n",
    "\n",
    "tt=tokenizer(random_sample[\"text\"],return_tensors=\"pt\", padding=True, truncation=True) # Tokenize random sample\n",
    "tt.to(device) # move input to GPU\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs=model(**tt)\n",
    "\n",
    "print(outputs)\n",
    "\n",
    "predictions = F.softmax(outputs.logits, dim=-1) # Turn logits into predictions\n",
    "\n",
    "print(\"Prediction for random sample:\" , (np.argmax(predictions.cpu())).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEFT (Parameter Efficient Fine-Tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Create LoRa configuration with rank=16\n",
    "lora_config = LoraConfig(\n",
    "    r=16, #RANK\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function that prints the number of trainable paramters\n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params=0\n",
    "    all_model_params=0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        # If parameter isn't frozen\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model\\n parameters:{trainable_model_params}\\n all model parameters {all_model_params}\\n percentrage of trainable model: {trainable_model_params/all_model_params*100}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model\n",
      " parameters:593669\n",
      " all model parameters 108907786\n",
      " percentrage of trainable model: 0.5451116231487801\n"
     ]
    }
   ],
   "source": [
    "original_model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)\n",
    "\n",
    "# Create the LoRa model ready for training \n",
    "lora_model = get_peft_model(original_model, lora_config)\n",
    "print(print_number_of_trainable_model_parameters(lora_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define TrainingArguments and create Trainer for LoRa\n",
    "lora_training_args = TrainingArguments(\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate= 1e-3,#higher learning rate than full fine-tuning\n",
    "    num_train_epochs=3, #increase for more accuracy\n",
    "    logging_steps=100,\n",
    "    report_to = \"none\",\n",
    "    output_dir=\"test_trainer\",\n",
    ")\n",
    "\n",
    "lora_trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=lora_training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7500/7500 30:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.600200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.407200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.192000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.243100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.151900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.147900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.160300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.142200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.093200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.175300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.154500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.098200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.110400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.064000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.077800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.144700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.064100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.118000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.137900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.102200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.101500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.095600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.054600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.021200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.059800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.973300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.037300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.069500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.036200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.036800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.041100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.015100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.995300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.010700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.030800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.024100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.014300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.997800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.975700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.957500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.991300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.979900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.947900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.938900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.975600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.946500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.863600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.921100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.895100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.916800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.907800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.885100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.887400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.889500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.932700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.887900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.891800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.938400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.919300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.855500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.824000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.898800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.878300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.830400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.851500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.841800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.888200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.819700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.848900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.844500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7500, training_loss=1.017799365234375, metrics={'train_runtime': 1810.7208, 'train_samples_per_second': 33.136, 'train_steps_per_second': 4.142, 'total_flos': 1.589651361792e+16, 'train_loss': 1.017799365234375, 'epoch': 3.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the LoRa model\n",
    "lora_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Save the LoRa fine-tuned model\n",
    "# lora_model.save_pretrained('./LoRa-fine-tuned-model')\n",
    "\n",
    "# Load the LoRa fine-tuned model\n",
    "lora_model = AutoModelForSequenceClassification.from_pretrained('./LoRa-fine-tuned-model', num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1/250 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9152711033821106,\n",
       " 'eval_model_preparation_time': 0.004,\n",
       " 'eval_accuracy': 0.609,\n",
       " 'eval_f1': 0.6092111956600339,\n",
       " 'eval_precision': 0.6117424650718493,\n",
       " 'eval_recall': 0.6076854474137148,\n",
       " 'eval_runtime': 28.3293,\n",
       " 'eval_samples_per_second': 70.598,\n",
       " 'eval_steps_per_second': 8.825}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model after training\n",
    "lora_metrics = lora_trainer.evaluate()\n",
    "lora_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QLoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "original_model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['quantization_dtype']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model\n",
      " parameters:593669\n",
      " all model parameters 108907786\n",
      " percentrage of trainable model: 0.5451116231487801\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# Again create LoRa model first\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "\n",
    "qlora_model = get_peft_model(original_model, lora_config)\n",
    "\n",
    "# Define quantization configuration\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # Load weights in 4-bit\n",
    "    quantization_dtype=torch.float16 # Calculations are made with float16\n",
    ")\n",
    "\n",
    "# Apply quantization configuration to the model\n",
    "qlora_model.quantization_config = quantization_config\n",
    "\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(qlora_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TrainingArguments and Trainer for QLoRa\n",
    "qlora_training_args = TrainingArguments(\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate= 1e-3,#higher learning rate than full fine-tuning\n",
    "    num_train_epochs=3, #increase for more accuracy\n",
    "    logging_steps=100,\n",
    "    report_to = \"none\",\n",
    "    output_dir=\"test_trainer\",\n",
    ")\n",
    "\n",
    "qlora_trainer = Trainer(\n",
    "    model=qlora_model,\n",
    "    args=qlora_training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7500/7500 30:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.470700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.204500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.199300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.124500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.147100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.178600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.109500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.103600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.144800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.099200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.171300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.124300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.123000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.072700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.037100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.102600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.120800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.091500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.123400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.069100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.021700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.086800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.022700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.010500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.091300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.964100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.987400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.034700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.020300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.013800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.016700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.963800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.006500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.952400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.993700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.060300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.012200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.003400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.004200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.969100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.009700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.948400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.953800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.963100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.961200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.926700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.936600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.944100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.877900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.874400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.925700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.908100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.901000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.851700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.889200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.904500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.887300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.893200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.904600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.875900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.839900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.856000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.887600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.863700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.804200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.810300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.812400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.887900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.806900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.828700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.853900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7500, training_loss=0.9949314270019531, metrics={'train_runtime': 1830.9957, 'train_samples_per_second': 32.769, 'train_steps_per_second': 4.096, 'total_flos': 1.589651361792e+16, 'train_loss': 0.9949314270019531, 'epoch': 3.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train QLoRa model\n",
    "qlora_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Save the QLoRa fine-tuned model\n",
    "# qlora_model.save_pretrained('./QLoRa-fine-tuned-model')\n",
    "\n",
    "# Load pretrained model\n",
    "qlora_model = AutoModelForSequenceClassification.from_pretrained('./QLoRa-fine-tuned-model', num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1/250 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9152711033821106,\n",
       " 'eval_model_preparation_time': 0.0038,\n",
       " 'eval_accuracy': 0.609,\n",
       " 'eval_f1': 0.6092111956600339,\n",
       " 'eval_precision': 0.6117424650718493,\n",
       " 'eval_recall': 0.6076854474137148,\n",
       " 'eval_runtime': 28.2819,\n",
       " 'eval_samples_per_second': 70.717,\n",
       " 'eval_steps_per_second': 8.84}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model after training\n",
    "qlora_metrics = qlora_trainer.evaluate()\n",
    "qlora_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Compare the metrics of all three trained models\n",
    "\n",
    "fig, ax = plt.subplots(1,3, figsize=(12, 3))\n",
    "sns.barplot(ax=ax[0], x=[\"accuracy\", \"f1\", \"precision\", \"recall\"], y=[fullfinetune_metrics[\"eval_accuracy\"], fullfinetune_metrics[\"eval_f1\"], fullfinetune_metrics[\"eval_precision\"], fullfinetune_metrics[\"eval_recall\"]], palette=\"viridis\")\n",
    "sns.barplot(ax=ax[1], x=[\"accuracy\", \"f1\", \"precision\", \"recall\"], y=[lora_metrics[\"eval_accuracy\"], lora_metrics[\"eval_f1\"], lora_metrics[\"eval_precision\"], lora_metrics[\"eval_recall\"]], palette=\"viridis\")\n",
    "sns.barplot(ax=ax[2], x=[\"accuracy\", \"f1\", \"precision\", \"recall\"], y=[qlora_metrics[\"eval_accuracy\"], qlora_metrics[\"eval_f1\"], qlora_metrics[\"eval_precision\"], qlora_metrics[\"eval_recall\"]], palette=\"viridis\")\n",
    "\n",
    "models = [\"Full FineTuning\", \"LoRa\", \"QLoRa\"]\n",
    "\n",
    "for i in range(3):\n",
    "    ax[i].set_ylim(0,1)\n",
    "    ax[i].set_title(models[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "LoRa and QLoRa achieved the same results as full Fine-Tuning although only less than 1% of all parameters were trained"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
